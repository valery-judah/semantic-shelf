# Incremental Evaluation Platform (docker-compose-first)

**Architecture, core concepts, rationale, and design principles**

This document describes an incremental evaluation platform for recommendation surfaces (starting with **Similar Books**) that runs locally with **docker-compose**, produces durable and diffable artifacts per run, and can later evolve to cloud-native execution without changing the evaluation interface.

## Table of contents

1. [Context and problem statement](#context-and-problem-statement)
2. [Goals, non-goals, and constraints](#goals-non-goals-and-constraints)
3. [Core concepts](#core-concepts)
4. [Design principles](#design-principles)
5. [Architecture overview](#architecture-overview)
6. [Storage model and schemas](#storage-model-and-schemas)
7. [Scenario design](#scenario-design)
8. [Load generator design](#load-generator-design)
9. [Evaluator design](#evaluator-design)
10. [Orchestration and workflow](#orchestration-and-workflow)
11. [CI integration and regression gating](#ci-integration-and-regression-gating)
12. [Incremental implementation plan](#incremental-implementation-plan)
13. [Appendix](#appendix)

## Context and problem statement

Recommendation iterations commonly fail in predictable ways:

- regressions are discovered late (post-deploy) or only via noisy online metrics,
- failures are hard to reproduce due to missing run boundaries and inconsistent inputs,
- debugging is slowed by ephemeral logs and ad-hoc local scripts.

The platform's goal is to create a **tight, credible feedback loop**:

- run a scenario (traffic + validations),
- collect raw signals with strong run identity,
- compute deterministic metrics and diagnostics,
- produce durable artifacts for review and CI gating.

## Goals, non-goals, and constraints
### Goals

- **Reproducible runs**: every evaluation run is tied to code, images, datasets, scenario versioning, and seeds.
- **Comparable results**: metrics are stored in a stable schema so runs can be diffed and trended.
- **Separation of concerns**: load generation, collection, evaluation, and reporting are distinct responsibilities.
- **Low-friction local execution**: everything works with docker-compose and shared volumes.
- **CI-ready outputs**: machine-readable summaries for regression gating + human-readable reports.

### Non-goals (v0/v1)

- A full experiment platform (long-lived randomization, allocation, online analysis pipelines).
- Production-grade observability as a prerequisite (metrics/tracing can be staged later).
- Perfect ranking quality measurement immediately (start with correctness/perf; add quality metrics incrementally).

### Constraint: docker-compose-first

The initial interface and contracts must work in local compose so engineers can iterate quickly and deterministically. Cloud-native execution is a later optimization and should preserve the same scenario + evaluator interface.

## Core concepts

These concepts are the backbone of reproducibility and comparability. They should be explicitly named in the repo, documented, and reflected in schemas.

### Run

A **Run** is a single evaluation execution with a globally unique identifier:

- `run_id`: unique per run, generated by the orchestrator.
- `run_id` is propagated end-to-end and used as the primary filter for all evaluation queries.

**Propagation**

- Every loadgen request includes:
  - `X-Eval-Run-Id: <run_id>`
  - `X-Request-Id: <request_id>` (unique per request)
- Telemetry events include `run_id=<run_id>` (or an equivalent stable field).

### Request

A **Request** is an individual API call (or logical request unit) within a run:

- `request_id`: unique per request.
- Requests are associated with:
  - scenario step name,
  - anchor identifier (e.g., `anchor_book_id`),
  - arm (baseline/candidate) if used,
  - timestamps and latency.

### Scenario

A **Scenario** is a versioned workload definition. It specifies:

- anchor selection / dataset,
- traffic model (duration, concurrency/QPS),
- steps (request templates + validations),
- optional telemetry emission,
- optional behavior model (synthetic interaction generation).

Scenarios are intended to be small and frequent (PR gating), with a path to larger nightly suites.

### Evaluator

An **Evaluator** is deterministic analytics code that:

- consumes run artifacts (and optionally telemetry rows filtered by `run_id`),
- computes metrics + diagnostics,
- emits:
  - `summary.json` (machine-readable),
  - `report.md` / `report.html` (human-readable),
  - optional charts and debug bundles.

Key constraint: evaluators compute from **stored artifacts**, not from "whatever logs happened to print".

### Artifact

An **Artifact** is a durable file produced per run and stored under a run-specific directory. Artifacts are the main reproducibility primitive:

- raw outputs from loadgen,
- evaluator summaries and reports,
- optional sampled requests/responses,
- optional telemetry extracts used by the evaluator.

### Telemetry event (optional early, essential later)

A **Telemetry event** is a structured record representing impressions, clicks, or outcomes for a surface.

- Early usage: support end-to-end plumbing and synthetic regressions.
- Later usage: compute CTR@K, position curves, and (eventually) outcome-based metrics.

### Baseline

A **Baseline** is the reference run used for regression detection:

- typically "last successful main/master run" for a given scenario (and optionally per-slice).
- stored as an artifact pointer or copied summary snapshot.

### Environment

Where the run executes:

- `compose`: docker-compose on the developer machine or CI runner.
- `terraform`/cloud: provisioned environment per run (future) while preserving the same run/scenario/evaluator interface.

## Design principles

1. **Run identity is non-negotiable**
   - If you can't filter signals to "this run only", you can't trust the metrics.
2. **Artifacts are the source of truth**
   - Durable artifacts beat ephemeral logs for reproducibility, reviewability, and CI automation.
3. **Stable schemas over ad-hoc parsing**
   - `run.json`, `summary.json`, and telemetry schemas should be versioned and validated.
4. **Separate generation from evaluation**
   - Loadgen generates traffic and writes raw outputs; evaluator computes metrics from those outputs.
5. **Incremental complexity**
   - Start with correctness + perf.
   - Add quality metrics only when data contracts exist (telemetry/judgments).
   - Add observability only after pain points are clear.
6. **Surface-agnostic evaluation core**
   - Common representation: `{impressions, interactions, outcomes}`.
   - Surface-specific config: event names, attribution windows, metric bundles.
7. **Debuggability is part of the contract**
   - Reports must include triage primitives (top failing anchors, worst latency samples, etc.), not only aggregates.
8. **Same interface locally and in CI**
   - If the local run is different from CI, engineers won't trust either.

## Architecture overview
### Architectural decomposition

The platform has three logical planes:

- **Control plane**: orchestrates runs (creates `run_id`, starts/stops services, triggers evaluators).
- **Data plane**: produces and stores raw signals (loadgen outputs, optional telemetry events).
- **Analytics plane**: computes metrics and produces reports from stored data.

### Component topology

```mermaid
flowchart LR
  subgraph ControlPlane[Control plane]
    ORCH[eval orchestrator CLI]
  end

  subgraph DataPlane[Data plane (compose)]
    LOAD[loadgen]
    APP[app (Similar Books API)]
    DB[(postgres)]
    TEL[/telemetry endpoint/]
  end

  subgraph AnalyticsPlane[Analytics plane]
    EVAL[evaluator]
    ART[(artifact store\n./artifacts/eval/<run_id>)]
  end

  ORCH -->|start services / set env| LOAD
  ORCH -->|trigger| EVAL

  LOAD -->|GET /v1/books/{id}/similar| APP
  APP --> DB
  LOAD -->|write raw outputs| ART

  LOAD -->|POST /telemetry/events (optional)| TEL
  TEL --> DB

  EVAL -->|read raw outputs| ART
  EVAL -->|query telemetry where run_id=run_id (optional)| DB
  EVAL -->|write summary + report| ART
```

Notes:

- In early stages you can omit a dedicated telemetry endpoint and use only loadgen outputs + API structured logs.
- Adding the telemetry endpoint/table enables quality metrics with a stable schema.

### Run lifecycle (high-level)

1. Orchestrator provisions environment (`compose up -d` or future cloud apply).
2. Orchestrator generates `run_id` and establishes run metadata.
3. Loadgen runs the scenario:
   - selects anchors,
   - executes steps,
   - validates responses,
   - writes raw outputs to artifacts,
   - optionally emits telemetry events.
4. Evaluator computes metrics from artifacts (and optional telemetry filtered by `run_id`).
5. Orchestrator collects and publishes outputs:
   - local: filesystem artifacts,
   - CI: upload artifacts + apply gating rules,
   - future: store in object storage with metadata index.

### "Minimal complete loop" (no new infra required)

A minimal loop can be:

- `loadgen` calls the API, validates, and writes a raw results file.
- `evaluator` reads raw results and produces `summary.json` and `report.md`.
- Artifacts live in `./artifacts/eval/<run_id>` for manual inspection and CI upload.

This loop already supports correctness and performance regressions.

## Storage model and schemas

This section describes what files/tables exist and why. It intentionally does not enumerate full JSON payloads; instead it describes required fields and invariants.

### Artifact directory layout

Per run:

```
./artifacts/eval/<run_id>/
  run.json
  raw/
    loadgen_results.json            # tool output (k6/locust/etc)
    validation_failures.jsonl        # structured failures (one per line)
    sample_requests/                # optional debug: request/response pairs
    telemetry_extract.jsonl         # optional: events used in evaluation
  summary/
    summary.json                    # machine-readable metrics and gate inputs
    deltas.json                     # optional: baseline vs candidate diffs
  report/
    report.md                       # human readable report
    charts/                         # optional images (latency hist, ctr curves)
  logs/                             # optional (only if useful)
```

Invariant:

- Artifacts are **append-only** within a run directory, and must not be modified after publish.

### `run.json` (run metadata)

Purpose: anchor reproducibility.

Typical fields (conceptual):

- identity:
  - `run_id`, `created_at`, `scenario_id`, `scenario_version`
- code and build:
  - `git_sha`, image tags/digests, evaluator version
- data:
  - dataset identifier/version, anchor selection method, seeds
- environment:
  - `env=compose|terraform`, host info, service versions
- configuration:
  - request limits, concurrency, durations, feature flags, algo/model IDs if applicable

Required properties:

- must uniquely identify code + config + data inputs for the run.
- should be schema-versioned, e.g. `run_schema_version`.

### `raw/loadgen_results.*` (traffic outputs)

Purpose: preserve the raw truth of what was executed.

Typical contents:

- request counts, status code distribution,
- latency distributions (p50/p95/p99) as produced by the load tool,
- error/timeouts, retries,
- optional per-request sample rows if the load tool supports it.

Guidance:

- preserve the native output (JSON/CSV) from k6/Locust where possible.
- avoid lossy aggregation in the load generator; do aggregation in evaluator.

### `raw/validation_failures.jsonl` (structured failures)

Purpose: deterministic correctness outcomes.

One line per failure with:

- `request_id`, anchor id, step name,
- failure type (status code, schema mismatch, duplicates, empty list, etc.),
- minimal context (e.g., offending JSONPath value),
- timestamps/latency.

### `summary/summary.json` (machine-readable evaluation output)

Purpose: CI gating + diffs.

Should contain:

- counts:
  - total requests, error rate, timeouts,
  - validation failures by type
- latency:
  - p50/p95/p99 (client-side; optionally server-side if captured)
- quality (when telemetry exists):
  - CTR@K, position curve summary, coverage proxy metrics
- diagnostics pointers:
  - filenames for top failures/worst anchors lists
- `summary_schema_version` (mandatory)

### Telemetry store schema (optional early; needed for CTR@K)

Two storage options:

1. **Postgres table** `telemetry_events` (recommended for simplicity and SQL joins)
2. **JSONL files** written by loadgen (acceptable for early iterations; harder to query robustly)

Minimal event properties (conceptual):

- identity:
  - `event_id` (or derived idempotency key), `event_name`, `ts`
- run/request:
  - `run_id`, `request_id`, `surface`
- content:
  - `anchor_id`, `shown_ids`, `positions`, optional `clicked_id`
- metadata:
  - `telemetry_schema_version`, `is_synthetic`, `arm`, `algo_id`, model/index versions

Important invariants:

- evaluator must be able to query **only** events for the run (`run_id`).
- events should be idempotent (avoid double writes from retries).
- schema must be versioned; new fields should be backward compatible when possible.

## Scenario design

Scenarios are the unit of evaluation. A good scenario is:

- **small** enough for PR gating,
- **stable** enough to reduce noise,
- **diagnostic** enough to localize failures quickly.

### Scenario structure (conceptual DSL)

A scenario definition should include:

1. **Identity**
   - `scenario_id`, `scenario_version`, `owner`, `description`
2. **Anchors / dataset**
   - how anchor books are selected:
     - static list file (golden set),
     - DB query,
     - deterministic sampling with seed,
     - stratified sampling by slice (head/torso/tail, language, genre).
   - anchor count and selection seed.
3. **Traffic model**
   - `duration_s`, `concurrency` and/or `qps`,
   - ramp-up and steady-state phases (optional),
   - per-anchor pacing or randomization policy.
4. **Steps**
   Steps are ordered and can depend on prior outputs:
   - request template (method, path, headers, query params)
   - validations (status codes, JSON constraints)
   - optional telemetry emission
   - optional extraction of fields for later steps
5. **Validation rules**
   Validations should be typed and deterministic:
   - HTTP status semantics (`200/400/404` behavior),
   - schema constraints (JSONPath exists, array length, numeric ranges),
   - list constraints (no duplicates, no anchor returned),
   - determinism checks for fixed `(anchor_id, algo_id, recs_version)` if exposed.
6. **Behavior model (optional)**
   Used to generate synthetic clicks/outcomes for plumbing tests:
   - position-based click model,
   - item-biased models (e.g., prefer "same author"),
   - explicit labeling: `is_synthetic=true`.

**Key rule:** synthetic events are valid for regression plumbing but should never be presented as product-quality evidence unless clearly separated.

### Scenario patterns for Similar Books
#### A) Smoke scenario (PR gating)

Purpose: contract correctness + basic performance.

Typical characteristics:

- short duration (1-3 minutes),
- limited anchors (100-500),
- moderate concurrency,
- strict validations.

Example validations:

- response contains `similar_book_ids`,
- list has no duplicates,
- list does not contain the anchor id,
- length matches `limit` when possible; otherwise assert acceptable fallback behavior.

#### B) Performance scenario (stress-ish)

Purpose: detect latency regressions under a stable load pattern.

Typical characteristics:

- longer duration (5-15 minutes),
- higher concurrency/QPS,
- optional warm-up period,
- record p50/p95/p99 and error rate.

#### C) Quality-ready scenario (telemetry-enabled)

Purpose: produce impression/click datasets for CTR@K and position curves.

Typical characteristics:

- emits `similar_impression` events for every response,
- emits `similar_click` events from either:
  - real interaction data replay (later), or
  - synthetic behavior model (early).

### Recommended additions (even if implemented later)
#### Arm-based execution (baseline vs candidate)

Support two "arms" within one run:

- `arm=baseline`: current default config
- `arm=candidate`: proposed change

This enables paired comparisons with reduced noise:

- same anchors, same timing window, same environment.

#### Slices

Scenarios should support slice definitions:

- head/torso/tail anchors,
- language/genre,
- cold-start proxies (few interactions),
- series vs standalone.

Evaluator should compute metrics per slice by default to surface localized regressions.

## Load generator design

Loadgen is responsible for executing scenarios, validating responses, and writing raw outputs and failures.

### Responsibilities

- generate `run_id` (or accept from orchestrator) and propagate headers
- sample/select anchors deterministically
- execute scenario steps with controlled concurrency/QPS
- validate each response with typed rules
- write raw results and structured failures to artifacts
- optionally emit telemetry events (impressions/clicks/outcomes)

### Tool choice

Common options:

- **k6**: great for load modeling and produces structured outputs.
- **Locust**: flexible Python logic for scenario steps and validations.

Either is fine as long as the evaluator can ingest its outputs deterministically.

### Required output contracts

At minimum, loadgen must produce:

- a summary output (native tool results JSON/CSV),
- a structured failures file with enough context to debug.

Optional but strongly recommended:

- a small sampled request/response bundle for the worst N anchors (as discovered by evaluator, or directly during load).

### Idempotency and retries

- request retries are fine, but must be visible in raw data (avoid hiding retries).
- telemetry emission should be idempotent:
  - derive an event key from `(run_id, request_id, event_name, arm)`.

### Boundary between loadgen and evaluator

- Loadgen should avoid computing "final metrics".
- Loadgen should produce raw facts; evaluator owns metric definitions.

## Evaluator design

Evaluator turns raw run artifacts into metrics, diagnostics, and a report.

### Inputs

- `run.json` (metadata)
- `raw/loadgen_results.*`
- `raw/validation_failures.jsonl`
- optional telemetry events for the run (DB query or JSONL extract)

### Outputs

- `summary/summary.json` (machine-readable)
- `report/report.md` (human-readable)
- optional:
  - charts (latency histogram, CTR@K curves),
  - debug bundles (worst anchors, sampled payloads),
  - baseline diffs (`summary/deltas.json`)

### Evaluator pipeline (recommended structure)

1. **Load metadata**: read `run.json`, validate schema version.
2. **Ingest raw outputs**:
   - parse load tool results,
   - parse structured failures.
3. **Compute correctness metrics**:
   - total failures, breakdown by type, top failing anchors.
4. **Compute performance metrics**:
   - p50/p95/p99, timeouts, error rate,
   - optionally server-side metrics if logged and extracted.
5. **Compute quality metrics** (when telemetry is available):
   - CTR@K, position curves,
   - NDCG@K if relevance signals exist (click-as-binary is acceptable as a first approximation),
   - coverage proxies (unique items, catalog coverage, diversity).
6. **Slice metrics** (if slice definitions exist):
   - compute all metrics per slice; report worst slice regressions.
7. **Diff against baseline** (optional early; essential for CI gating):
   - compare `summary.json` to baseline summary for the same scenario.
8. **Emit outputs**:
   - write `summary.json` and `report.md`,
   - write auxiliary diagnostics files referenced by the report.

### Metric bundles (Similar Books)
#### A) Contract + correctness (always-on)

- status code distribution
- schema checks and invariants:
  - list exists,
  - no duplicates,
  - anchor not included,
  - sensible list length / fallback behavior
- empty-result rate (if applicable)
- determinism checks if supported by the API versioning model

These are cheap and should run on every PR.

#### B) Performance under load

- p50/p95/p99 latency (client-side from load tool output)
- error rate, timeout rate
- fallback rate if exposed (e.g., "used fallback index")

Guidance:

- define "steady state" windows (exclude warm-up) if your workload needs it.
- keep the perf scenario stable; change it only intentionally.

#### C) Quality (telemetry-enabled)

From `similar_impression` and `similar_click`:

- CTR@K
- position curves (CTR by position)
- optional NDCG@K (click as relevance=1) as a "directional" measure
- coverage proxies:
  - unique recommended IDs / run,
  - unique authors / run (if metadata available),
  - diversity proxies (intra-list similarity is a later enhancement).

### Diagnostics and triage primitives

Reports should include:

- top failing anchors (by failure count/type),
- "worst latency" anchors with sample payload pointers,
- counts of 404/400 and other non-200 statuses,
- distribution plots where helpful (latency histograms, CTR curves).

A report that cannot be used to debug is not sufficient for CI gating.

### Determinism and statistical considerations

- Prefer paired baselines (baseline + candidate arms in same run) to reduce noise.
- If you compute deltas, provide:
  - absolute delta and relative delta,
  - simple confidence bounds (paired bootstrap) when feasible.

## Orchestration and workflow

The orchestrator is intentionally small and boring.

### Responsibilities

- create `run_id`
- start/stop compose services (or provision cloud in the future)
- run loadgen with scenario parameters
- run evaluator
- ensure artifacts are placed under the correct run directory
- optionally maintain baseline pointers

### Recommended interface

- `eval run --scenario <id> --env compose`
- `eval report --run <run_id>`
- `eval diff --baseline <run_id> --candidate <run_id>` (optional)

### docker-compose integration

Use compose profiles so evaluation services can be enabled on demand:

- base services: `app`, `db`
- eval profile services: `loadgen`, `evaluator` (and optionally telemetry)

Example invocation pattern:

- local: `docker compose --profile eval up -d`
- run: `make eval-similar-smoke`
- teardown: `docker compose down -v` (optional in CI)

### Advanced Execution (Stage 4)

**Running with Golden Sets**

To use a stable, versioned set of anchors (reducing noise), specify the golden set ID:

```bash
./scripts/eval_run.sh similar_books_smoke --dataset-id similar_books_smoke_v1
```

**Running Paired Arms**

To compare a candidate against a baseline within the same run (reducing environmental noise), use a scenario configured with `paired_arms: true`:

```bash
./scripts/eval_run.sh similar_books_paired --dataset-id similar_books_smoke_v1
```

This will produce a report with:
- Per-slice metrics (head/tail, language, etc.)
- Paired delta analysis (candidate vs baseline latency and correctness)

## CI integration and regression gating
### CI job outline

A single CI job can:

1. build images
2. `docker compose up -d`
3. seed DB
4. run a short scenario (smoke)
5. run evaluator
6. upload artifacts
7. apply gating rules (diff against baseline)
8. teardown compose

### Gating rules (practical defaults)

Hard gates:

- correctness failures > 0
- p95 latency regression > X% (relative) or > Y ms (absolute)
- error/timeout rate regression above threshold

Soft gates (warnings or "requires review"):

- CTR@K deltas when clicks are synthetic or low volume
- diversity/coverage proxy regressions

### Baseline selection

Typical baseline strategies:

- "last successful main run for this scenario"
- "most recent nightly run for this scenario"
- per-slice baselines if slice metrics are used

Store baseline pointers as:

- artifact metadata in CI (e.g., a pointer file),
- or a small metadata index (SQLite/Postgres) later.

## Incremental implementation plan

This plan is organized around **capabilities** (what you can trust and act on) rather than around components. Each stage is a closed loop that produces durable artifacts and a concrete "decision surface" (pass/fail, diff, gate) with minimal new infrastructure.

### Stage 0 -- Evaluation contract and run identity (foundation)

**Objective:** Everything produced during evaluation is unambiguously attributable to a run and request.

**Deliverables**

- **Run identity**
  - orchestrator generates `run_id`
  - loadgen generates/assigns `request_id` per request
  - `X-Eval-Run-Id` and `X-Request-Id` headers are added to every request
  - app logs structured fields containing `run_id`, `request_id`, scenario/step, and latency
- **Artifact layout**
  - create run-scoped directory `./artifacts/eval/<run_id>/...`
  - reserve paths for `run.json`, `raw/`, `summary/`, `report/`
- **Schema discipline**
  - define `run.json` and `summary.json` *schemas* (even if `summary.json` is minimal)
  - add `*_schema_version` fields and schema validation in evaluator
- **Determinism hooks**
  - plumb scenario version and seed into run metadata (even if scenario is embedded code initially)

**Exit criteria**

- You can answer: "Which code/config/dataset/scenario produced this report?" from files alone.
- A run can be rerun with the same scenario + seed and produce the same *inputs* (anchors, request templates).

**Failure modes to avoid**

- missing `run_id` in logs/telemetry
- mutable artifacts (edited after the fact)
- unversioned schemas (you'll break diffs later)

### Stage 1 -- Single smoke scenario producing raw artifacts (first usable loop)

**Objective:** A repeatable, deterministic **correctness** signal for Similar Books.

**Deliverables**

- **One scenario**: `similar_books_smoke` (short, stable)
  - anchors: deterministic selection (static list or deterministic DB query + seed)
  - traffic: small (e.g., 1-3 min), bounded concurrency/QPS
  - validations: strict response invariants (status codes, schema, duplicates, anchor-not-in-list, min length/fallback rules)
- **Raw outputs**
  - persist native load tool output (`raw/loadgen_results.json` or CSV)
  - persist structured failures (`raw/validation_failures.jsonl`)
  - persist a small sample set for debugging (optional early; required by Stage 2)

**Exit criteria**

- Engineers can run `make eval-similar-smoke` locally and get deterministic pass/fail.
- Failures are attributable to anchor + validation type + minimal context (not just "failed").

**Notes**

- Don't block on telemetry; correctness first.

### Stage 2 -- Evaluator v1: deterministic analytics + triage primitives

**Objective:** Convert raw outputs into actionable summaries and debug-friendly reports.

**Deliverables**

- **Evaluator container/service** that:
  - reads `run.json`, validates schema versions
  - ingests raw load outputs + failure JSONL
  - emits `summary/summary.json` with:
    - request counts, status distribution, error/timeout rate
    - validation failures by type
    - latency p50/p95/p99 (client-side)
  - emits `report/report.md` with triage sections:
    - top failing anchors (by count/type)
    - worst latency anchors
    - status code breakdown and timeouts
    - links/pointers to raw files
- **Debug bundles**
  - write `raw/sample_requests/` for worst N anchors (request + response + headers + timestamps)
  - report references these samples explicitly

**Exit criteria**

- A regression is debuggable in minutes from the report:
  - "what failed", "where", "example payload", "who owns scenario"
- `summary.json` is stable enough to diff between runs (schema-versioned).

**Failure modes to avoid**

- reports with only aggregates and no examples
- metrics computed from logs instead of artifacts (logs are non-deterministic)

### Stage 3 -- Baseline selection + diff + CI gating (make it enforceable)

**Objective:** Prevent regressions from landing by turning evaluation into a CI control.

**Deliverables**

- **Baseline strategy** (documented and implemented):
  - "last successful main run for scenario_id" as default baseline
  - store baseline pointer (artifact pointer file or CI metadata)
- **Diff tool**:
  - compares `summary.json` candidate vs baseline
  - emits `summary/deltas.json` with absolute and relative deltas
- **CI integration**
  - run smoke scenario on PRs
  - upload artifacts
  - apply gating rules:
    - hard: correctness failures > 0
    - hard: p95 regression beyond threshold (absolute or relative)
    - hard: error/timeout rate regression beyond threshold
    - soft: warn-only changes (reserved for later quality signals)

**Exit criteria**

- A PR cannot merge if it breaks correctness or blows latency thresholds.
- CI output links to the run's report and artifacts.

**Notes**

- Keep gating conservative: only gate on low-ambiguity signals at this stage.

### Stage 4 -- Noise control: golden set, slices, and paired arms

**Objective:** Reduce variance and make regressions localizable.

**Deliverables**

- **Golden anchor sets**
  - stable, versioned lists (e.g., 200-1k anchors) for PR gating
  - optionally multiple goldens per slice (head/torso/tail, language)
- **Slice framework**
  - `slices.yaml` describing slice membership or rules
  - evaluator computes all core metrics per slice by default
  - report highlights worst slice regressions even if overall is flat
- **Paired baseline/candidate arms within one run**
  - for each anchor, run both arms and store under same `run_id`
  - evaluator produces paired deltas to reduce noise

**Exit criteria**

- Metrics variance is low enough that small regressions are detectable reliably.
- "Overall flat but slice regressed" becomes visible and actionable.

**Failure modes to avoid**

- comparing separate runs without controlling anchors/slices (high noise)
- slice metrics computed inconsistently between runs

### Stage 5 -- Performance scenario hardening (steady-state discipline)

**Objective:** Reliable latency regression detection with stable load modeling.

**Deliverables**

- **Dedicated perf scenario** `similar_books_perf`
  - warm-up phase, steady-state window, explicit ramping rules
  - concurrency/QPS held stable across runs
- **Perf-focused evaluator outputs**
  - steady-state p50/p95/p99
  - timeouts/error rate
  - optional separate "server-side" latency if extracted reliably
- **Gating updates**
  - perf gating in CI may run less frequently (e.g., nightly) if runtime is longer
  - PR gating can keep smoke-only; perf gating can be optional/label-triggered early

**Exit criteria**

- Perf scenario produces consistent latency distributions across clean runs.
- Latency regression alerts correlate with real user-facing slowdowns (as verified once).

### Stage 6 -- Telemetry contract and storage (enable quality metrics safely)

**Objective:** Establish a trustworthy data contract for impressions/clicks attributable to evaluation runs.

**Deliverables**

- **Telemetry schema v1**
  - versioned event schema (`telemetry_schema_version`)
  - explicit `run_id`, `request_id`, `surface`, `arm`, and timestamps
  - idempotency key to avoid double counting
  - explicit `is_synthetic` marker for generated events
- **Storage**
  - `telemetry_events` table in Postgres (recommended) with indices:
    - `(run_id)`, `(run_id, event_name)`, `(request_id)`
  - or JSONL event store (acceptable if evaluator performance is fine)
- **Evaluator integration**
  - evaluator can query events for `run_id` only
  - evaluator writes `raw/telemetry_extract.jsonl` (optional) for reproducibility

**Exit criteria**

- You can compute CTR@K from evaluation telemetry and reproduce it offline from stored artifacts/exports.
- Synthetic vs real telemetry is separated in report and summary.

**Failure modes to avoid**

- "JSON blob" events with no stable fields (hard to query/diff)
- missing idempotency leading to inflated CTR

### Stage 7 -- Quality metrics v1: offline proxies first, CTR@K as a guarded signal

**Objective:** Provide quality signal without overclaiming, and without depending solely on online metrics.

**Deliverables**

- **Offline proxy metrics** (no clicks required), e.g.:
  - metadata agreement rates (author/series/genre/language overlap)
  - diversity/coverage guardrails (unique authors/items, concentration measures)
  - stability metrics (overlap@K across builds for a fixed golden set)
- **CTR@K and position curves** (telemetry-enabled)
  - computed only when event volume and attribution are adequate
  - treated as soft gate until proven stable and representative
- **Reporting upgrades**
  - "top regressed anchors by quality delta"
  - slice-level quality deltas
  - optional paired bootstrap confidence bounds when using paired arms

**Exit criteria**

- Quality metrics correlate with manual spot checks on regressed anchors.
- The platform can detect a quality regression in at least one historically known failure case.

**Notes**

- If you add judged sets (human relevance labels), introduce them here and promote NDCG@K to a first-class metric.

### Stage 8 -- Scale-out and observability (only after demonstrated pain)

**Objective:** Operational maturity: trends, retention, and scalable execution, without changing the evaluation interface.

**Deliverables**

- **Artifact storage**
  - move artifacts to object storage (preserve directory layout)
  - metadata index for run discovery (SQLite/Postgres)
- **Nightly suites**
  - larger scenarios, more slices, longer perf runs
  - trend dashboards over `summary.json` outputs
- **Optional observability**
  - Prometheus/Grafana/OTel in compose or cloud
  - service-level metrics for app, loadgen, evaluator

**Exit criteria**

- Engineers can answer: "When did this metric start regressing?" from trends.
- Storage/retention is defined and automated.

### Stage progression rules (meta-principles)

- **Do not advance stages until the previous stage is trusted.** "Exists" is not equal to "used".
- **Gate only on low-ambiguity signals** until data contracts and variance are controlled.
- **Prefer paired comparisons** and golden sets to reduce noise before interpreting quality deltas.
- **Keep scenario definitions small and versioned**; stability beats breadth early.

## Appendix
### Suggested docker-compose service set (local end state)

- `app` (API)
- `db` (Postgres)
- `loadgen` (k6/Locust) -- enabled in `eval` profile
- `evaluator` -- enabled in `eval` profile
- optional: `otel-collector`, `prometheus`, `grafana` -- enabled in an `obs` profile

### Suggested repo structure

```
/scenarios/
  similar_books_smoke.yaml
  similar_books_perf.yaml
  similar_books_quality.yaml
/eval/
  orchestrator/
  evaluator/
  schemas/
  reporting/
/scripts/
  eval_run.sh (optional wrapper)
/artifacts/ (gitignored)
```

### Telemetry table (conceptual guidance)

If using Postgres, create a single `telemetry_events` table with:

- indexed `run_id`
- indexed `(run_id, event_name)`
- optional idempotency unique key on `(run_id, request_id, event_name, arm)`

Keep `payload_json` only if you must; prefer typed columns for common fields used in evaluators (anchor id, shown ids, clicked id, positions).
